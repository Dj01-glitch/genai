# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I3xVtfpCQKda5ZuLDRdbEL9rqKmr08sw
"""

# Basic libraries
import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import warnings
warnings.filterwarnings('ignore')
from math import ceil
from collections import defaultdict
from tqdm.notebook import tqdm        # Progress bar library for Jupyter Notebook

# Deep learning framework for building and training models
import tensorflow as tf
## Pre-trained model for image feature extraction
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array

## Tokenizer class for captions tokenization
from tensorflow.keras.preprocessing.text import Tokenizer

## Function for padding sequences to a specific length
from tensorflow.keras.preprocessing.sequence import pad_sequences

## Class for defining Keras models
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate, Bidirectional, Dot, Activation, RepeatVector, Multiply, Lambda

# For checking score
from nltk.translate.bleu_score import corpus_bleu

import os
import kagglehub
flickr_path = kagglehub.dataset_download('durjoychatterjee/flickr')
# Setting the input and output directory
INPUT_DIR = '/kaggle/input/flickr'
OUTPUT_DIR = '/kaggle/working/'
print(os.listdir(INPUT_DIR))

# We are going to use pretraind vgg model
# Load the vgg16 model
model = VGG16()

# Restructuring the model to remove the last classification layer, this will give us access to the output features of the model
model = Model(inputs=model.inputs, outputs=model.layers[-2].output)

# Printing the model summary
print(model.summary())

# Initialize an empty dictionary to store image features
image_features = {}

# Define the directory path where images are located
img_dir = os.path.join(INPUT_DIR, 'Images')

# Loop through each image in the directory
for img_name in tqdm(os.listdir(img_dir)):
    # Load the image from file
    img_path = os.path.join(img_dir, img_name)
    image = load_img(img_path, target_size=(224, 224))
    # Convert image pixels to a numpy array
    image = img_to_array(image)
    # Reshape the data for the model
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
    # Preprocess the image for ResNet50
    image = preprocess_input(image)
    # Extract features using the pre-trained ResNet50 model
    image_feature = model.predict(image, verbose=0)
    # Get the image ID by removing the file extension
    image_id = img_name.split('.')[0]
    # Store the extracted feature in the dictionary with the image ID as the key
    image_features[image_id] = image_feature

import os
import pickle
# Store the image features in pickle
os.makedirs(OUTPUT_DIR, exist_ok=True)
pickle.dump(image_features, open(os.path.join(OUTPUT_DIR, 'img_features.pkl'), 'wb'))

# Load features from pickle file
pickle_file_path = os.path.join(OUTPUT_DIR, 'img_features.pkl')
with open(pickle_file_path, 'rb') as file:
    loaded_features = pickle.load(file)

with open(os.path.join(INPUT_DIR, 'captions.txt'), 'r') as file:
    next(file)
    captions_doc = file.read()

# Create mapping of image to captions
image_to_captions_mapping = defaultdict(list)

# Process lines from captions_doc
for line in tqdm(captions_doc.split('\n')):
    # Split the line by comma(,)
    tokens = line.split(',')
    if len(tokens) < 2:
        continue
    image_id, *captions = tokens
    # Remove extension from image ID
    image_id = image_id.split('.')[0]
    # Convert captions list to string
    caption = " ".join(captions)
    # Store the caption using defaultdict
    image_to_captions_mapping[image_id].append(caption)

# Print the total number of captions
total_captions = sum(len(captions) for captions in image_to_captions_mapping.values())
print("Total number of captions:", total_captions)

# Function for processing the captions
def clean(mapping):
    for key, captions in mapping.items():
        for i in range(len(captions)):
            # Take one caption at a time
            caption = captions[i]
            # Preprocessing steps
            # Convert to lowercase
            caption = caption.lower()
            # Remove non-alphabetical characters
            caption = ''.join(char for char in caption if char.isalpha() or char.isspace())
            # Remove extra spaces
            caption = caption.replace('\s+', ' ')
            # Add unique start and end tokens to the caption
            caption = 'startseq ' + ' '.join([word for word in caption.split() if len(word) > 1]) + ' endseq'
            captions[i] = caption

# before preprocess of text
image_to_captions_mapping['1026685415_0431cbf574']

# preprocess the text
clean(image_to_captions_mapping)

image_to_captions_mapping['1026685415_0431cbf574']

# Creating a List of All Captions
all_captions = [caption for captions in image_to_captions_mapping.values() for caption in captions]

all_captions[:10]

# Tokenizing the Text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
# Save the tokenizer
with open('tokenizer.pkl', 'wb') as tokenizer_file:
    pickle.dump(tokenizer, tokenizer_file)

# Load the tokenizer
with open('tokenizer.pkl', 'rb') as tokenizer_file:
    tokenizer = pickle.load(tokenizer_file)
# Calculate maximum caption length
max_caption_length = max(len(tokenizer.texts_to_sequences([caption])[0]) for caption in all_captions)
vocab_size = len(tokenizer.word_index) + 1

# Print the results
print("Vocabulary Size:", vocab_size)
print("Maximum Caption Length:", max_caption_length)

# Creating a List of Image IDs
image_ids = list(image_to_captions_mapping.keys())
# Splitting into Training and Test Sets
split = int(len(image_ids) * 0.90)
train = image_ids[:split]
test = image_ids[split:]
# Data generator function
def data_generator(data_keys, image_to_captions_mapping, features, tokenizer, max_caption_length, vocab_size, batch_size):
    # Lists to store batch data
    X1_batch, X2_batch, y_batch = [], [], []
    # Counter for the current batch size
    batch_count = 0

    while True:
        # Loop through each image in the current batch
        for image_id in data_keys:
            # Get the captions associated with the current image
            captions = image_to_captions_mapping[image_id]

            # Loop through each caption for the current image
            for caption in captions:
                # Convert the caption to a sequence of token IDs
                caption_seq = tokenizer.texts_to_sequences([caption])[0]

                # Loop through the tokens in the caption sequence
                for i in range(1, len(caption_seq)):
                    # Split the sequence into input and output pairs
                    in_seq, out_seq = caption_seq[:i], caption_seq[i]

                    # Pad the input sequence to the specified maximum caption length
                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]

                    # Convert the output sequence to one-hot encoded format
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]

                    # Append data to batch lists
                    X1_batch.append(features[image_id][0])  # Image features
                    X2_batch.append(in_seq)  # Input sequence
                    y_batch.append(out_seq)  # Output sequence

                    # Increase the batch counter
                    batch_count += 1

                    # If the batch is complete, yield the batch and reset lists and counter
                    if batch_count == batch_size:
                        X1_batch, X2_batch, y_batch = np.array(X1_batch), np.array(X2_batch), np.array(y_batch)
                        yield [X1_batch, X2_batch], y_batch
                        X1_batch, X2_batch, y_batch = [], [], []
                        batch_count = 0

# Encoder model
inputs1 = Input(shape=(4096,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
fe2_projected = RepeatVector(max_caption_length)(fe2)
fe2_projected = Bidirectional(LSTM(256, return_sequences=True))(fe2_projected)

# Sequence feature layers
inputs2 = Input(shape=(max_caption_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = Bidirectional(LSTM(256, return_sequences=True))(se2)

# Apply attention mechanism using Dot product
attention = Dot(axes=[2, 2])([fe2_projected, se3])  # Calculate attention scores

# Softmax attention scores
attention_scores = Activation('softmax')(attention)

# Apply attention scores to sequence embeddings
attention_context = Lambda(lambda x: tf.einsum('ijk,ijl->ikl', x[0], x[1]))([attention_scores, se3])

# Sum the attended sequence embeddings along the time axis
# Wrap tf.reduce_sum in a Lambda layer
context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(attention_context)

# Decoder model
decoder_input = concatenate([context_vector, fe2], axis=-1)
decoder1 = Dense(256, activation='relu')(decoder_input)
outputs = Dense(vocab_size, activation='softmax')(decoder1)

# Create the model
model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# Visualize the model
plot_model(model, show_shapes=True)

# Data generator function
import tensorflow as tf  # Import TensorFlow

def data_generator(data_keys, image_to_captions_mapping, features, tokenizer, max_caption_length, vocab_size, batch_size):
    # Lists to store batch data
    X1_batch, X2_batch, y_batch = [], [], []
    # Counter for the current batch size
    batch_count = 0

    while True:
        # Loop through each image in the current batch
        for image_id in data_keys:
            # Get the captions associated with the current image
            captions = image_to_captions_mapping[image_id]

            # Loop through each caption for the current image
            for caption in captions:
                # Convert the caption to a sequence of token IDs
                caption_seq = tokenizer.texts_to_sequences([caption])[0]

                # Loop through the tokens in the caption sequence
                for i in range(1, len(caption_seq)):
                    # Split the sequence into input and output pairs
                    in_seq, out_seq = caption_seq[:i], caption_seq[i]

                    # Pad the input sequence to the specified maximum caption length
                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]

                    # Convert the output sequence to one-hot encoded format
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]

                    # Append data to batch lists
                    X1_batch.append(features[image_id][0])  # Image features
                    X2_batch.append(in_seq)  # Input sequence
                    y_batch.append(out_seq)  # Output sequence

                    # Increase the batch counter
                    batch_count += 1

                    # If the batch is complete, yield the batch and reset lists and counter
                    if batch_count == batch_size:
                        X1_batch, X2_batch, y_batch = np.array(X1_batch), np.array(X2_batch), np.array(y_batch)

                        # Define the output signature using tf.TensorSpec
                        output_signature = (
                            (tf.TensorSpec(shape=X1_batch.shape, dtype=tf.float32),  # Image features
                             tf.TensorSpec(shape=X2_batch.shape, dtype=tf.int32)),  # Input sequence
                            tf.TensorSpec(shape=y_batch.shape, dtype=tf.float32)  # Output sequence
                        )

                        yield [X1_batch, X2_batch], y_batch
                        X1_batch, X2_batch, y_batch = [], [], []
                        batch_count = 0

# Save the model
model.save(OUTPUT_DIR+'/mymodel.h5')

from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from tqdm import tqdm
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Helper: get word from index
def get_word_from_index(index, tokenizer):
    return next((word for word, idx in tokenizer.word_index.items() if idx == index), None)

# Beam search decoder
def beam_search_prediction(model, image_features, tokenizer, max_length, beam_index=3):
    start = [tokenizer.word_index['startseq']]
    sequences = [[start, 0.0]]

    while len(sequences[0][0]) < max_length:
        all_candidates = []
        for seq, score in sequences:
            sequence = pad_sequences([seq], maxlen=max_length)
            yhat = model.predict([image_features, sequence], verbose=0)
            top_candidates = np.argsort(yhat[0])[-beam_index:]
            for idx in top_candidates:
                word = get_word_from_index(idx, tokenizer)
                if word is None:
                    continue
                new_seq = seq + [idx]
                new_score = score + np.log(yhat[0][idx] + 1e-10)
                all_candidates.append([new_seq, new_score])
        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_index]

    final_sequence = sequences[0][0]
    caption = [get_word_from_index(idx, tokenizer) for idx in final_sequence]
    return ' '.join(caption)

# Evaluation loop
actual_captions_list = []
predicted_captions_list = []

for key in tqdm(test):
    # Actual captions: list of reference captions
    actual = image_to_captions_mapping[key]  # List of strings
    actual_cleaned = [[word for word in caption.split() if word not in ('startseq', 'endseq')] for caption in actual]

    # Predicted caption using beam search
    predicted = beam_search_prediction(model, loaded_features[key], tokenizer, max_caption_length, beam_index=5)
    predicted_cleaned = [word for word in predicted.split() if word not in ('startseq', 'endseq')]

    actual_captions_list.append(actual_cleaned)
    predicted_captions_list.append(predicted_cleaned)

# Calculate BLEU score with smoothing
smooth = SmoothingFunction().method4
print("BLEU-1: %f" % corpus_bleu(actual_captions_list, predicted_captions_list, weights=(1.0, 0, 0, 0), smoothing_function=smooth))
print("BLEU-2: %f" % corpus_bleu(actual_captions_list, predicted_captions_list, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth))

# Function for generating caption
def generate_caption(image_name):
    # load the image
    image_id = image_name.split('.')[0]
    img_path = os.path.join(INPUT_DIR, "Images", image_name)
    image = Image.open(img_path)
    captions = image_to_captions_mapping[image_id]
    print('---------------------Actual---------------------')
    for caption in captions:
        print(caption)
    # predict the caption
    y_pred = predict_caption(model, loaded_features[image_id], tokenizer, max_caption_length)
    print('--------------------Predicted--------------------')
    print(y_pred)
    plt.imshow(image)
generate_caption("101669240_b2d3e7f17b.jpg")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from PIL import Image
# import matplotlib.pyplot as plt
# import os
# import io
# 
# # ======= Load Required Artifacts (Assuming they are already defined) =======
# # These should be defined and loaded beforehand
# # model, tokenizer, max_caption_length, loaded_features, image_to_captions_mapping
# 
# INPUT_DIR = "/kaggle/input/flickr"  # You can update this if needed
# 
# # Dummy example functions â€” Replace with your actual implementations
# def predict_caption(model, feature, tokenizer, max_length):
#     return "This is a predicted caption."  # Replace with actual logic
# 
# # ========== Caption Generation Logic ========== #
# def generate_caption_streamlit(uploaded_image, uploaded_filename):
#     image_id = uploaded_filename.split('.')[0]
#     image = Image.open(uploaded_image).convert("RGB")
# 
#     st.image(image, caption="ðŸ“· Uploaded Image", use_column_width=True)
# 
#     # Show Actual Captions
#     st.markdown("### âœ… Actual Captions")
#     captions = image_to_captions_mapping.get(image_id, ["Not found in mapping"])
#     for i, cap in enumerate(captions):
#         st.markdown(f"{i+1}. {cap}")
# 
#     # Show Predicted Caption
#     st.markdown("### ðŸ¤– Predicted Caption")
#     if image_id in loaded_features:
#         y_pred = predict_caption(model, loaded_features[image_id], tokenizer, max_caption_length)
#         st.success(y_pred)
#     else:
#         st.warning("Image features not found! Make sure the image was in the training set.")
# 
# # ========== Streamlit UI ========== #
# st.set_page_config(page_title="Image Captioning", page_icon="ðŸ–¼ï¸", layout="centered")
# 
# st.title("ðŸ–¼ï¸ AI Image Captioning")
# st.markdown("Upload an image to view actual and predicted captions.")
# 
# uploaded_file = st.file_uploader("ðŸ“‚ Upload an image", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file is not None:
#     try:
#         uploaded_filename = uploaded_file.name
#         with st.spinner("ðŸ” Analyzing the image..."):
#             generate_caption_streamlit(uploaded_file, uploaded_filename)
#     except Exception as e:
#         st.error(f"Error: {e}")
# else:
#     st.info("ðŸ‘† Upload an image to get started.")
# 
# st.markdown(
#     "<br><hr><p style='text-align: center; color: gray;'>ðŸš€ GenAI Project</p>",
#     unsafe_allow_html=True
# )
#